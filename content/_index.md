---
title: LLM Security
date: 2022-09-24
toc: false

image:
  filename: covers/pexels-nuno-fangueiro-12125258.jpg
  caption:  Monstera - Nuno Fangueiro
---

LLM security is the investigation of the failure modes of LLMs in use, the conditions that lead to them, and their mitigations.

Here are links to large language model security content - research, papers, and news - posted by [@llm_sec](https://twitter.com/llm_sec)

Got a tip/link? Open a [pull request](https://github.com/leondz/llmsec-site) or send a [DM](https://twitter.com/llm_sec).

## Adversarial

* [A LLM Assisted Exploitation of AI-Guardian](https://arxiv.org/abs/2307.15008)
* [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/abs/1905.02175)
* [Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation in Language Models](https://arxiv.org/abs/2307.12507)

## Backdoors

* [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.14710)

## Prompt injection
* [Compromising LLMs: The Advent of AI Malware](https://www.blackhat.com/us-23/briefings/schedule/index.html#compromising-llms-the-advent-of-ai-malware-33075)
* [Hackers Compromised ChatGPT Model with Indirect Prompt Injection](https://gbhackers.com/hackers-compromised-chatgpt-model/)

## Denial of service

## Cross-model

* [Exploring the Vulnerability of Natural Language Processing Models via Universal Adversarial Texts](https://aclanthology.org/2021.alta-1.14/)

## Multimodal

* [Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models](https://arxiv.org/abs/2307.14539)

## Policy, legal, ethical, and social

* [On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey](https://arxiv.org/abs/2307.16680)
* [The last attempted AI revolution in security, and the next one](https://drive.google.com/file/d/1BbSIBayQ1RHVSnh-FnaeXr8xjw5SVJV8/view?pli=1)
* [Unveiling Security, Privacy, and Ethical Concerns of ChatGPT](https://arxiv.org/abs/2307.14192)

## Software

* [garak](https://github.com/leondz/garak/) LLM vulnerability scanner

## Mitigations

* [Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias](https://arxiv.org/abs/2305.04547)
* [ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP](https://arxiv.org/abs/2308.02122)